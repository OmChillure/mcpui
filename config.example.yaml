port: 8080
logLevel: info # Choose one of the following: debug, info, warn, error, default to info
logMode: text # Choose one of the following: json, text, default to text
systemPrompt: You are a helpful assistant.
titleGeneratorPrompt: Generate a title for this chat with only one sentence with maximum 5 words.
# Choose one of the following LLM providers: ollama, anthropic
llm:
  provider: ollama
  model: claude-3-5-sonnet-20241022
  parameters: # This is optional, and only used by some LLM providers.
    temperature: 0.5
    topP: 0.9
    topK: 40
    frequencyPenalty: 0.0
    presencePenalty: 0.0
    repetitionPenalty: 1.0
    minP: 0.0
    topA: 0.0
    seed: 0
    maxTokens: 1000
    logitBias: 
      dummyTokenID: 0.0
    logprobs: true
    topLogprobs: 10
    stop:
      - "\n"
      - "\n\n"
    includeReasoning: true
  # ollama
  host: http://localhost:11434 # Default to environment variable OLLAMA_HOST
  # anthropic
  apiKey: YOUR_API_KEY # Default to environment variable ANTHROPIC_API_KEY
  maxTokens: 1000
  # openai
  apiKey: YOUR_API_KEY # Default to environment variable OPENAI_API_KEY
  # openrouter
  apiKey: YOUR_API_KEY # Default to environment variable OPENROUTER_API_KEY
genTitleLLM: # Default to the same LLM as the main LLM
  provider: anthropic
  model: claude-3-5-sonnet-20241022
  apiKey: YOUR_API_KEY # Default to environment variable ANTHROPIC_API_KEY
  maxTokens: 1000
  parameters: # This is optional, and only used by some LLM providers.
    temperature: 0.5
    topP: 0.9
    topK: 40
    frequencyPenalty: 0.0
    presencePenalty: 0.0
    repetitionPenalty: 1.0
    minP: 0.0
    topA: 0.0
    seed: 0
    maxTokens: 1000
    logprobs: true
    topLogprobs: 10
    stop:
      - "\n"
      - "\n\n"
    includeReasoning: true
mcpSSEServers:
  filesystem:
    url: https://yoursseserver.com
    maxPayloadSize: 1048576 # 1MB
mcpStdIOServers:
  filesystem:
    command: npx
    args:
      - -y
      - "@modelcontextprotocol/server-filesystem"
      - "/home/gs/repository/go-mcp"
